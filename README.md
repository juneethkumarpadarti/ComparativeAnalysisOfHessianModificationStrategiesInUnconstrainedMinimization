# ComparativeAnalysisOfHessianModificationStrategiesInUnconstrainedMinimization

## ğŸ“Œ Overview

  This project presents a comprehensive comparative study of three Hessian modification strategies in the context of unconstrained optimization. Using Newton's Method as the base algorithm, the work focuses on ensuring the Hessian matrix remains positive definite, which is crucial for convergence efficiency and stability.

The three strategies analyzed are:

1. Eigenvalue Shifting

2. Diagonal Adjustment & Cholesky Factorization

3. Eigenvalue Clamping

The performance of each method was evaluated on 11 benchmark optimization functions of varying complexity and dimensionality.

## ğŸ§  Problem Statement

  Newton's Method relies on the Hessian matrix being positive definite for stable convergence. However, in many optimization problems, the Hessian may fail this condition, causing slow convergence or divergence.
This project compares three Hessian modification strategies to determine:

- Which strategy most effectively ensures positive definiteness.

- Which strategy converges fastest (fewest iterations and function evaluations).

- How strategies perform across simple, moderate, and complex function landscapes.

## ğŸ” Methodology

- Optimization Algorithm: Newton's Method with Hessian modification.

- Step Size Control: Line search to balance progress and stability.

- Convergence Criteria: Norm of gradient < tolerance, or maximum iteration limit.

**Hessian Handling:**

  Strategy 1: Eigenvalue Shifting

  Strategy 2: Diagonal Adjustment & Cholesky Factorization

  Strategy 3: Eigenvalue Clamping

  Test Functions (11 total):

  Rosenbrock (n), Beale, Matyas, Powell Singular, Sphere (n), Booth, Styblinskiâ€“Tang (n), McCormick, Easom, Wood, Helical Valley

## ğŸ“Š Key Findings

- Strategy 2 (Diagonal Adjustment & Cholesky) emerged as the most adaptable and consistently efficient, especially for complex landscapes like Powell Singular.

- Strategy 1 was stable but conservative, often requiring more iterations and evaluations.

- Strategy 3 was faster in some cases but less consistent than Strategy 2 in reaching low gradient norms.

Simpler functions (Sphere, Matyas, Easom) showed minimal differences between strategies, while complex, multi-modal functions benefited more from Strategy 2â€™s adaptiveness.

## ğŸ“ˆ Results Summary

| Function            | Best Strategy| Notable Observations                              |
|---------------------|--------------|---------------------------------------------------|
| Rosenbrock          | 2 / 3        | Large performance variation                       |
| Beale               | 2            | Fast convergence                                  |
| Powell Singular     | 2            | Fewest iterations, most efficient                 |
| Styblinskiâ€“Tang     | 2 / 3        | Strategy 2 better for gradient norm               |
| Wood, Helical Valley| Mixed        | All strategies hit step tolerance quickly         |


## ğŸ’¡ Insights

  - Efficiency: Strategy 2 balanced convergence speed and robustness.
  
  - Adaptability: Better performance in landscapes with steep gradients and multiple minima.
  
  - Scalability: Works well across varying function dimensions.

## ğŸ› ï¸ Tech & Math Stack

  Language: Python
  
  Core Concepts: Newton's Method, Positive Definite Hessians, Eigenvalue Decomposition, Cholesky Factorization
  
  Numerical Libraries: NumPy, SciPy
  
  Visualization: Matplotlib (for convergence plots)

## ğŸ“‚ Repository Structure

  <summary>Project Directory Structure</summary>

```plaintext
â”œâ”€â”€ data/                 # Test functions & configurations
â”œâ”€â”€ src/                  # Implementation of Hessian modification strategies & optimization code
â”œâ”€â”€ results/              # Output data, plots, and logs
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ report.pdf            # Full project report with methodology & analysis
```
## ğŸ“œ References

- Numerical Optimization by Jorge Nocedal & Stephen Wright
- Python documentation on optimization functions
